# ğŸ“Š Data Pipeline Using Kafka & Hadoop HDFS (Step-by-Step Guide)

This project demonstrates a **real-world data pipeline** where data is collected from **Twitter**, streamed through **Apache Kafka**, and finally stored in **Hadoop HDFS** running on a **Virtual Machine**.

This README explains:

* The **architecture**
* **Installation & configuration** steps
* **How to run the project step by step**
* **All problems encountered** and **how we solved them** (very important for learning & interviews)

---

## ğŸ—ï¸ Architecture Overview

```
Twitter API
   â†“
Kafka Producer (Python â€“ Windows Host)
   â†“
Kafka Topic (twitter_data)
   â†“
Kafka Consumer (Python â€“ Windows Host)
   â†“
HDFS (Hadoop 2.x on Ubuntu Virtual Machine)
```

### Key Points

* Kafka & Python run on **Windows (Host)**
* Hadoop HDFS runs on **Ubuntu (VirtualBox VM)**
* Communication happens via **network IP (not localhost)**

---

## ğŸ§° Technologies Used

* **Python 3.13**
* **Apache Kafka 3.8.1** (Windows)
* **Apache Hadoop 2.7.x** (Ubuntu VM)
* **Twitter API v2 (Tweepy)**
* **VirtualBox**

---

## ğŸ“ Project Structure

```
Data-Pipeline-using-Kafka-and-Hadoop-HDFS/
â”‚
â”œâ”€â”€ kafka_tweet_preducer.py      # Twitter â†’ Kafka producer
â”œâ”€â”€ kafka_tweet_consumer.py      # Kafka â†’ Console consumer (test)
â”œâ”€â”€ hdfs_consumer.py             # Kafka â†’ HDFS consumer
â”œâ”€â”€ secret.txt                   # Twitter API keys
â”œâ”€â”€ test.py                      # HDFS connection test
â””â”€â”€ README.md
```

---

## ğŸ–¥ï¸ Step 1 â€“ Setup Hadoop HDFS (Ubuntu VM)

### 1ï¸âƒ£ Install Hadoop on Ubuntu

* Hadoop 2.7.x installed in:

```
/home/vboxuser/Desktop/hadoop2/hadoop-2.7.3
```

### 2ï¸âƒ£ Check HDFS Services

```bash
jps
```

Expected:

```
NameNode
DataNode
SecondaryNameNode
```

### 3ï¸âƒ£ HDFS Web UI

From **Windows browser**:

```
http://<VM_IP>:50070
```

Example:

```
http://192.168.1.106:50070
```

---

## ğŸŒ Step 2 â€“ Network Configuration (VERY IMPORTANT)

### VM IP Address

```bash
ip a
```

Example:

```
eth0 â†’ 192.168.1.106
```

â¡ï¸ This IP **must be used everywhere** (never `localhost`).

---

## âš™ï¸ Step 3 â€“ Hadoop Configuration Files

### âœ… core-site.xml

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://192.168.1.106:9000</value>
  </property>
</configuration>
```

### âœ… hdfs-site.xml (FINAL FIXED VERSION)

```xml
<configuration>

  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/vboxuser/Desktop/hdfs/namenode</value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/vboxuser/Desktop/hdfs/datanode</value>
  </property>

  <!-- Force IP usage instead of hostname -->
  <property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>false</value>
  </property>

  <property>
    <name>dfs.datanode.use.datanode.hostname</name>
    <value>false</value>
  </property>

  <!-- Bind DataNode to all interfaces -->
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:50010</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:50075</value>
  </property>

</configuration>
```

### Restart HDFS

```bash
stop-dfs.sh
start-dfs.sh
```

---

## ğŸªŸ Step 4 â€“ Windows DNS Fix (CRITICAL)

### âŒ Problem

Hadoop returns this hostname:

```
ubuntu.myguest.virtualbox.org
```

Windows **cannot resolve it**, causing HDFS writes to fail.

### âœ… Solution â€“ Edit Windows hosts file

Open **Notepad as Administrator** and edit:

```
C:\Windows\System32\drivers\etc\hosts
```

Add:

```
192.168.1.106   ubuntu.myguest.virtualbox.org
```

Flush DNS:

```powershell
ipconfig /flushdns
```

---

## ğŸ§ª Step 5 â€“ Test HDFS from Windows

```powershell
python test.py
```

Expected output:

```
WRITE SUCCESS
```

Verify on VM:

```bash
hdfs dfs -cat /twitter_data/test.txt
```

---

## ğŸ§µ Step 6 â€“ Setup Kafka (Windows)

Go to Kafka directory:

```powershell
cd C:\kafka\kafka_2.13-3.8.1
```

### Start Zookeeper

```powershell
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties
```

### Start Kafka Broker

```powershell
.\bin\windows\kafka-server-start.bat .\config\server.properties
```

### Create Topic

```powershell
.\bin\windows\kafka-topics.bat --create --topic twitter_data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

---

## ğŸ Step 7 â€“ Run the Pipeline

### Terminal 1 â€“ HDFS Consumer

```powershell
python hdfs_consumer.py
```

### Terminal 2 â€“ Twitter Producer

```powershell
python kafka_tweet_preducer.py
```

Expected output:

```
Stored 50 tweets in /twitter_data/tweets_YYYYMMDD_HHMMSS.json
```

Verify in HDFS:

```bash
hdfs dfs -ls /twitter_data
```

---

## âš ï¸ Common Problems & Solutions

### âŒ UnicodeEncodeError on Windows

**Cause:** Unicode arrow character `â†’`

**Fix:**

```python
print("Kafka -> HDFS consumer started...")
```

---

### âŒ HDFS Connection Error

**Cause:** Hadoop hostname not resolvable on Windows

**Fix:** Add hostname to `hosts` file

---

### âŒ Kafka commands not recognized

**Cause:** Using `.sh` scripts on Windows

**Fix:** Use `.bat` scripts

---

## ğŸ“ Interview-Ready Explanation

> â€œI built a Kafka-based streaming pipeline where data is ingested from Twitter, streamed via Kafka, and persisted in HDFS running on a virtual machine. I solved real-world issues such as Hadoop hostname resolution, Windows encoding errors, and cross-OS networking.â€

---

## ğŸš€ Conclusion

This project demonstrates:

* Real-time data streaming
* Distributed storage with HDFS
* Cross-platform integration
* Real production-level debugging

âœ… **Pipeline fully functional**

---

## ğŸ“Œ Author

**ESSEDIK MOHAMMED**

Feel free to fork, improve, or extend this project ğŸš€
